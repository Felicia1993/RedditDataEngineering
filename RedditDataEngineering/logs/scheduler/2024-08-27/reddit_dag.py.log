[2024-08-27T18:23:58.076+0000] {processor.py:186} INFO - Started process (PID=27) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:23:58.077+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:23:58.078+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:23:58.078+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:23:58.086+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:23:58.085+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:23:58.086+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:23:58.100+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.029 seconds
[2024-08-27T18:24:28.416+0000] {processor.py:186} INFO - Started process (PID=28) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:24:28.417+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:24:28.419+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:24:28.419+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:24:28.427+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:24:28.426+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:24:28.427+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:24:28.449+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-08-27T18:24:58.734+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:24:58.735+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:24:58.738+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:24:58.737+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:24:58.744+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:24:58.743+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:24:58.745+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:24:58.762+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.036 seconds
[2024-08-27T18:25:29.037+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:25:29.038+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:25:29.040+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:25:29.040+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:25:29.047+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:25:29.046+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:25:29.047+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:25:29.066+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.034 seconds
[2024-08-27T18:25:59.409+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:25:59.410+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:25:59.413+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:25:59.413+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:25:59.420+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:25:59.419+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:25:59.421+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:25:59.439+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.035 seconds
[2024-08-27T18:26:29.764+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:26:29.765+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:26:29.767+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:26:29.766+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:26:29.773+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:26:29.772+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:26:29.774+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:26:29.796+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-08-27T18:27:00.123+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:27:00.124+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:27:00.126+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:27:00.126+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:27:00.133+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:27:00.132+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:27:00.133+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:27:00.149+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.033 seconds
[2024-08-27T18:27:30.573+0000] {processor.py:186} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:27:30.575+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:27:30.579+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:27:30.578+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:27:30.587+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:27:30.586+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:27:30.587+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:27:30.607+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T18:28:01.002+0000] {processor.py:186} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:28:01.003+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:28:01.006+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:28:01.006+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:28:01.014+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:28:01.013+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:28:01.014+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:28:01.034+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.041 seconds
[2024-08-27T18:28:31.443+0000] {processor.py:186} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:28:31.445+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:28:31.448+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:28:31.447+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:28:31.456+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:28:31.455+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:28:31.457+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:28:31.479+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T18:29:01.824+0000] {processor.py:186} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:29:01.826+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:29:01.829+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:29:01.829+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:29:01.839+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:29:01.838+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:29:01.840+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:29:01.863+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.050 seconds
[2024-08-27T18:29:32.212+0000] {processor.py:186} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:29:32.214+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:29:32.216+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:29:32.215+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:29:32.221+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:29:32.220+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:29:32.221+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:29:32.235+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.032 seconds
[2024-08-27T18:30:02.584+0000] {processor.py:186} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:30:02.585+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:30:02.587+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:30:02.587+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:30:02.593+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:30:02.592+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:30:02.593+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:30:02.608+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:30:32.982+0000] {processor.py:186} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:30:32.984+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:30:32.987+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:30:32.987+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:30:32.996+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:30:32.995+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:30:32.997+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:30:33.020+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.047 seconds
[2024-08-27T18:31:03.357+0000] {processor.py:186} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:31:03.359+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:31:03.362+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:31:03.361+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:31:03.370+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:31:03.369+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:31:03.371+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:31:03.392+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:31:33.725+0000] {processor.py:186} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:31:33.727+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:31:33.732+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:31:33.731+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:31:33.741+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:31:33.740+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:31:33.742+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:31:33.760+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:32:04.103+0000] {processor.py:186} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:32:04.104+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:32:04.107+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:32:04.107+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:32:04.116+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:32:04.115+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:32:04.117+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:32:04.139+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.046 seconds
[2024-08-27T18:32:34.496+0000] {processor.py:186} INFO - Started process (PID=44) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:32:34.498+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:32:34.501+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:32:34.501+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:32:34.510+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:32:34.508+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:32:34.510+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:32:34.532+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.046 seconds
[2024-08-27T18:33:04.862+0000] {processor.py:186} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:33:04.863+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:33:04.866+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:33:04.866+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:33:04.875+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:33:04.874+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:33:04.876+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:33:04.897+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:33:35.242+0000] {processor.py:186} INFO - Started process (PID=46) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:33:35.243+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:33:35.246+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:33:35.246+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:33:35.255+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:33:35.254+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:33:35.256+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:33:35.277+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:34:05.643+0000] {processor.py:186} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:34:05.645+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:34:05.649+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:34:05.648+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:34:05.657+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:34:05.656+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:34:05.658+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:34:05.679+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T18:34:36.040+0000] {processor.py:186} INFO - Started process (PID=48) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:34:36.041+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:34:36.044+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:34:36.044+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:34:36.052+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:34:36.051+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:34:36.053+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:34:36.075+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:35:06.427+0000] {processor.py:186} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:35:06.428+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:35:06.431+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:35:06.431+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:35:06.441+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:35:06.440+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:35:06.442+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:35:06.463+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:35:36.830+0000] {processor.py:186} INFO - Started process (PID=50) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:35:36.831+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:35:36.835+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:35:36.834+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:35:36.843+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:35:36.842+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:35:36.844+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:35:36.864+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T18:36:07.211+0000] {processor.py:186} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:36:07.212+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:36:07.215+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:36:07.214+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:36:07.223+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:36:07.222+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:36:07.223+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:36:07.244+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.041 seconds
[2024-08-27T18:36:37.597+0000] {processor.py:186} INFO - Started process (PID=52) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:36:37.599+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:36:37.600+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:36:37.600+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:36:37.606+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:36:37.605+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:36:37.607+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:36:37.622+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.034 seconds
[2024-08-27T18:37:07.991+0000] {processor.py:186} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:37:07.992+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:37:07.996+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:37:07.995+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:37:08.004+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:37:08.003+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:37:08.005+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:37:08.027+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T18:37:38.393+0000] {processor.py:186} INFO - Started process (PID=54) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:37:38.395+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:37:38.398+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:37:38.397+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:37:38.407+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:37:38.405+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:37:38.407+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:37:38.429+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T18:38:08.828+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:38:08.829+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:38:08.831+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:38:08.831+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:38:08.840+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:38:08.839+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:38:08.841+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:38:08.861+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.040 seconds
[2024-08-27T18:38:39.233+0000] {processor.py:186} INFO - Started process (PID=56) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:38:39.234+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:38:39.238+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:38:39.237+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:38:39.246+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:38:39.244+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:38:39.246+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:38:39.267+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.042 seconds
[2024-08-27T18:39:09.784+0000] {processor.py:186} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:39:09.786+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:39:09.788+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:39:09.788+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:39:09.797+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:39:09.795+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:39:09.798+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:39:09.819+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.042 seconds
[2024-08-27T18:39:40.227+0000] {processor.py:186} INFO - Started process (PID=58) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:39:40.228+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:39:40.230+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:39:40.230+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:39:40.236+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:39:40.235+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:39:40.237+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:39:40.253+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.032 seconds
[2024-08-27T18:40:10.628+0000] {processor.py:186} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:40:10.629+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:40:10.632+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:40:10.632+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:40:10.640+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:40:10.639+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:40:10.641+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:40:10.662+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.042 seconds
[2024-08-27T18:40:41.064+0000] {processor.py:186} INFO - Started process (PID=60) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:40:41.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:40:41.069+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:40:41.068+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:40:41.078+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:40:41.076+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:40:41.078+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:40:41.104+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.048 seconds
[2024-08-27T18:41:11.470+0000] {processor.py:186} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:41:11.472+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:41:11.476+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:41:11.475+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:41:11.485+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:41:11.483+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:41:11.485+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:41:11.506+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T18:41:41.869+0000] {processor.py:186} INFO - Started process (PID=62) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:41:41.871+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:41:41.873+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:41:41.873+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:41:41.883+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:41:41.881+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:41:41.883+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:41:41.906+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:42:12.230+0000] {processor.py:186} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:42:12.232+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:42:12.236+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:42:12.235+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:42:12.244+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:42:12.243+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:42:12.245+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:42:12.266+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.046 seconds
[2024-08-27T18:42:42.641+0000] {processor.py:186} INFO - Started process (PID=64) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:42:42.643+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:42:42.646+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:42:42.645+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:42:42.654+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:42:42.653+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:42:42.654+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:42:42.675+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T18:43:13.013+0000] {processor.py:186} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:43:13.013+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:43:13.015+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:43:13.015+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:43:13.019+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:43:13.018+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:43:13.020+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:43:13.032+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.023 seconds
[2024-08-27T18:43:43.361+0000] {processor.py:186} INFO - Started process (PID=66) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:43:43.363+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:43:43.366+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:43:43.365+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:43:43.373+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:43:43.372+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:43:43.374+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:43:43.390+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.036 seconds
[2024-08-27T18:44:13.645+0000] {processor.py:186} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:44:13.646+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:44:13.647+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:44:13.647+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:44:13.652+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:44:13.651+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:44:13.653+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:44:13.668+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:44:43.874+0000] {processor.py:186} INFO - Started process (PID=68) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:44:43.874+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:44:43.876+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:44:43.876+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:44:43.882+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:44:43.881+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:44:43.882+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:44:43.897+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:45:14.096+0000] {processor.py:186} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:45:14.096+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:45:14.098+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:45:14.098+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:45:14.103+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:45:14.102+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:45:14.103+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:45:14.118+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.026 seconds
[2024-08-27T18:45:44.303+0000] {processor.py:186} INFO - Started process (PID=70) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:45:44.303+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:45:44.305+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:45:44.305+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:45:44.310+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:45:44.309+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:45:44.311+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:45:44.326+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:46:14.521+0000] {processor.py:186} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:46:14.522+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:46:14.524+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:46:14.524+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:46:14.529+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:46:14.528+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:46:14.529+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:46:14.544+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:46:44.725+0000] {processor.py:186} INFO - Started process (PID=72) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:46:44.725+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:46:44.727+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:46:44.727+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:46:44.732+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:46:44.731+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:46:44.732+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:46:44.748+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:47:14.965+0000] {processor.py:186} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:47:14.966+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:47:14.968+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:47:14.967+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:47:14.974+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:47:14.973+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:47:14.975+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:47:14.989+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:47:45.180+0000] {processor.py:186} INFO - Started process (PID=74) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:47:45.181+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:47:45.183+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:47:45.183+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:47:45.189+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:47:45.188+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:47:45.189+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:47:45.203+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:48:15.397+0000] {processor.py:186} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:48:15.398+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:48:15.399+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:48:15.399+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:48:15.405+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:48:15.404+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:48:15.405+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:48:15.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:48:45.611+0000] {processor.py:186} INFO - Started process (PID=76) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:48:45.612+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:48:45.613+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:48:45.613+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:48:45.619+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:48:45.618+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:48:45.619+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:48:45.634+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.026 seconds
[2024-08-27T18:49:15.850+0000] {processor.py:186} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:49:15.851+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:49:15.853+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:49:15.852+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:49:15.859+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:49:15.858+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:49:15.859+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:49:15.873+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:49:46.064+0000] {processor.py:186} INFO - Started process (PID=78) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:49:46.065+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:49:46.067+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:49:46.066+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:49:46.073+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:49:46.072+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:49:46.073+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:49:46.087+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:50:16.338+0000] {processor.py:186} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:50:16.339+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:50:16.340+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:50:16.340+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:50:16.346+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:50:16.345+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:50:16.346+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:50:16.361+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:50:46.622+0000] {processor.py:186} INFO - Started process (PID=80) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:50:46.622+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:50:46.624+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:50:46.624+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:50:46.629+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:50:46.628+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:50:46.629+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:50:46.644+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:51:16.937+0000] {processor.py:186} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:51:16.938+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:51:16.940+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:51:16.940+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:51:16.946+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:51:16.945+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:51:16.946+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:51:16.960+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:51:47.209+0000] {processor.py:186} INFO - Started process (PID=82) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:51:47.210+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:51:47.211+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:51:47.211+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:51:47.217+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:51:47.216+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:51:47.217+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:51:47.232+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:52:17.436+0000] {processor.py:186} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:52:17.436+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:52:17.438+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:52:17.437+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:52:17.443+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:52:17.442+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:52:17.443+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:52:17.459+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:52:47.682+0000] {processor.py:186} INFO - Started process (PID=84) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:52:47.683+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:52:47.685+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:52:47.684+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:52:47.690+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:52:47.689+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:52:47.691+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:52:47.705+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.026 seconds
[2024-08-27T18:53:17.942+0000] {processor.py:186} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:53:17.943+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:53:17.946+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:53:17.945+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:53:17.951+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:53:17.950+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:53:17.952+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:53:17.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:53:48.195+0000] {processor.py:186} INFO - Started process (PID=86) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:53:48.196+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:53:48.198+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:53:48.198+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:53:48.203+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:53:48.202+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:53:48.203+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:53:48.218+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:54:18.471+0000] {processor.py:186} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:54:18.472+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:54:18.474+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:54:18.473+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:54:18.479+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:54:18.478+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:54:18.479+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:54:18.495+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:54:48.747+0000] {processor.py:186} INFO - Started process (PID=88) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:54:48.747+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:54:48.749+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:54:48.749+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:54:48.754+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:54:48.753+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:54:48.754+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:54:48.769+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:55:19.009+0000] {processor.py:186} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:55:19.010+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:55:19.012+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:55:19.011+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:55:19.018+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:55:19.017+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:55:19.019+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:55:19.032+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T18:55:49.263+0000] {processor.py:186} INFO - Started process (PID=90) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:55:49.264+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:55:49.266+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:55:49.266+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:55:49.271+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:55:49.271+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:55:49.272+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:55:49.286+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:56:19.538+0000] {processor.py:186} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:56:19.539+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:56:19.541+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:56:19.540+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:56:19.546+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:56:19.545+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:56:19.546+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:56:19.561+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:56:49.803+0000] {processor.py:186} INFO - Started process (PID=92) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:56:49.804+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:56:49.806+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:56:49.806+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:56:49.812+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:56:49.811+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:56:49.813+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:56:49.827+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.032 seconds
[2024-08-27T18:57:20.029+0000] {processor.py:186} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:57:20.030+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:57:20.032+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:57:20.032+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:57:20.038+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:57:20.037+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:57:20.038+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:57:20.052+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:57:50.296+0000] {processor.py:186} INFO - Started process (PID=94) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:57:50.296+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:57:50.298+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:57:50.298+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:57:50.303+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:57:50.302+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:57:50.303+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:57:50.319+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:58:20.512+0000] {processor.py:186} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:58:20.513+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:58:20.514+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:58:20.514+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:58:20.520+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:58:20.519+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:58:20.521+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:58:20.535+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:58:50.743+0000] {processor.py:186} INFO - Started process (PID=96) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:58:50.744+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:58:50.746+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:58:50.746+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:58:50.751+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:58:50.751+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:58:50.752+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:58:50.766+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T18:59:20.983+0000] {processor.py:186} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:59:20.984+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:59:20.986+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:59:20.985+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:59:20.991+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:59:20.990+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:59:20.991+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:59:21.006+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.026 seconds
[2024-08-27T18:59:51.228+0000] {processor.py:186} INFO - Started process (PID=98) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:59:51.229+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T18:59:51.231+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:59:51.231+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:59:51.237+0000] {logging_mixin.py:190} INFO - [2024-08-27T18:59:51.236+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T18:59:51.238+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T18:59:51.251+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T19:00:21.501+0000] {processor.py:186} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:00:21.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:00:21.504+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:00:21.504+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:00:21.510+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:00:21.509+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:00:21.510+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:00:21.524+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T19:00:51.765+0000] {processor.py:186} INFO - Started process (PID=100) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:00:51.765+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:00:51.767+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:00:51.767+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:00:51.773+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:00:51.772+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:00:51.773+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:00:51.789+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.029 seconds
[2024-08-27T19:01:22.018+0000] {processor.py:186} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:01:22.019+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:01:22.020+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:01:22.020+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:01:22.025+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:01:22.025+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:01:22.026+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:01:22.041+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T19:01:52.282+0000] {processor.py:186} INFO - Started process (PID=102) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:01:52.282+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:01:52.284+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:01:52.284+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:01:52.290+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:01:52.289+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:01:52.290+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:01:52.305+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T19:02:22.560+0000] {processor.py:186} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:02:22.561+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:02:22.562+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:02:22.562+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:02:22.568+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:02:22.567+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:02:22.569+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:02:22.584+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.030 seconds
[2024-08-27T19:02:52.801+0000] {processor.py:186} INFO - Started process (PID=104) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:02:52.802+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:02:52.804+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:02:52.804+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:02:52.809+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:02:52.808+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:02:52.809+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:02:52.824+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T19:03:23.046+0000] {processor.py:186} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:03:23.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:03:23.049+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:03:23.049+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:03:23.054+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:03:23.053+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:03:23.054+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:03:23.068+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.026 seconds
[2024-08-27T19:03:53.325+0000] {processor.py:186} INFO - Started process (PID=106) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:03:53.326+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:03:53.327+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:03:53.327+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:03:53.333+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:03:53.332+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:03:53.333+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:03:53.346+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.025 seconds
[2024-08-27T19:04:23.623+0000] {processor.py:186} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:04:23.624+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:04:23.625+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:04:23.625+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:04:23.630+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:04:23.630+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:04:23.631+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:04:23.645+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.026 seconds
[2024-08-27T19:04:53.954+0000] {processor.py:186} INFO - Started process (PID=108) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:04:53.956+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:04:53.959+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:04:53.958+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:04:53.968+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:04:53.967+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:04:53.969+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:04:53.992+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.047 seconds
[2024-08-27T19:05:24.361+0000] {processor.py:186} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:05:24.362+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:05:24.366+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:05:24.366+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:05:24.375+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:05:24.374+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:05:24.376+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:05:24.400+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.047 seconds
[2024-08-27T19:05:54.739+0000] {processor.py:186} INFO - Started process (PID=110) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:05:54.740+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:05:54.743+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:05:54.743+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:05:54.751+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:05:54.750+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:05:54.751+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:05:54.771+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.040 seconds
[2024-08-27T19:06:25.119+0000] {processor.py:186} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:06:25.120+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:06:25.124+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:06:25.123+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:06:25.133+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:06:25.131+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:06:25.133+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:06:25.155+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:06:55.507+0000] {processor.py:186} INFO - Started process (PID=112) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:06:55.509+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:06:55.512+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:06:55.512+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:06:55.521+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:06:55.520+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:06:55.522+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:06:55.540+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:07:25.848+0000] {processor.py:186} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:07:25.849+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:07:25.852+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:07:25.852+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:07:25.860+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:07:25.859+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:07:25.861+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:07:25.882+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.042 seconds
[2024-08-27T19:07:56.183+0000] {processor.py:186} INFO - Started process (PID=114) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:07:56.185+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:07:56.188+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:07:56.188+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:07:56.198+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:07:56.197+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:07:56.198+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:07:56.219+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:08:26.583+0000] {processor.py:186} INFO - Started process (PID=115) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:08:26.585+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:08:26.589+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:08:26.588+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:08:26.597+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:08:26.596+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:08:26.598+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:08:26.619+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:08:57.012+0000] {processor.py:186} INFO - Started process (PID=116) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:08:57.013+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:08:57.016+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:08:57.016+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:08:57.025+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:08:57.023+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:08:57.025+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:08:57.046+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:09:27.451+0000] {processor.py:186} INFO - Started process (PID=117) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:09:27.453+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:09:27.456+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:09:27.456+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:09:27.465+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:09:27.464+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:09:27.465+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:09:27.488+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.046 seconds
[2024-08-27T19:09:57.867+0000] {processor.py:186} INFO - Started process (PID=118) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:09:57.869+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:09:57.871+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:09:57.871+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:09:57.880+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:09:57.879+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:09:57.881+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:09:57.902+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T19:10:28.239+0000] {processor.py:186} INFO - Started process (PID=119) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:10:28.240+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:10:28.243+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:10:28.243+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:10:28.252+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:10:28.251+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:10:28.253+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:10:28.273+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:10:58.675+0000] {processor.py:186} INFO - Started process (PID=120) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:10:58.676+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:10:58.679+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:10:58.678+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:10:58.685+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:10:58.684+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:10:58.686+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:10:58.707+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.038 seconds
[2024-08-27T19:11:29.055+0000] {processor.py:186} INFO - Started process (PID=121) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:11:29.056+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:11:29.058+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:11:29.058+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:11:29.065+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:11:29.064+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:11:29.066+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:11:29.085+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.038 seconds
[2024-08-27T19:11:59.495+0000] {processor.py:186} INFO - Started process (PID=122) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:11:59.497+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:11:59.500+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:11:59.499+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:11:59.509+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:11:59.508+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:11:59.510+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:11:59.531+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:12:29.892+0000] {processor.py:186} INFO - Started process (PID=123) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:12:29.894+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:12:29.897+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:12:29.896+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:12:29.906+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:12:29.905+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:12:29.906+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:12:29.927+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:13:00.274+0000] {processor.py:186} INFO - Started process (PID=124) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:13:00.276+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:13:00.279+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:13:00.278+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:13:00.287+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:13:00.286+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:13:00.287+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:13:00.311+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.047 seconds
[2024-08-27T19:13:30.663+0000] {processor.py:186} INFO - Started process (PID=125) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:13:30.664+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:13:30.667+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:13:30.666+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:13:30.675+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:13:30.674+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:13:30.675+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:13:30.694+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-08-27T19:14:01.016+0000] {processor.py:186} INFO - Started process (PID=126) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:14:01.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:14:01.020+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:14:01.019+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:14:01.025+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:14:01.024+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:14:01.025+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:14:01.041+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.030 seconds
[2024-08-27T19:14:31.408+0000] {processor.py:186} INFO - Started process (PID=127) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:14:31.409+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:14:31.412+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:14:31.412+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:14:31.423+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:14:31.421+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:14:31.423+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:14:31.445+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:15:01.874+0000] {processor.py:186} INFO - Started process (PID=128) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:15:01.876+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:15:01.880+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:15:01.879+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:15:01.889+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:15:01.888+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:15:01.890+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:15:01.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.048 seconds
[2024-08-27T19:15:32.312+0000] {processor.py:186} INFO - Started process (PID=129) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:15:32.313+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:15:32.317+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:15:32.316+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:15:32.326+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:15:32.325+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:15:32.327+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:15:32.352+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.049 seconds
[2024-08-27T19:16:02.746+0000] {processor.py:186} INFO - Started process (PID=130) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:02.748+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:16:02.751+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:16:02.750+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:02.761+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:16:02.760+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:16:02.762+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:02.785+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.048 seconds
[2024-08-27T19:16:33.121+0000] {processor.py:186} INFO - Started process (PID=131) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:33.122+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:16:33.125+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:16:33.124+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:33.132+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:16:33.131+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 28, in <module>
    python_callable=reddit_pipeline,
NameError: name 'reddit_pipeline' is not defined
[2024-08-27T19:16:33.132+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:33.151+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.037 seconds
[2024-08-27T19:16:49.365+0000] {processor.py:186} INFO - Started process (PID=132) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:49.366+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:16:49.369+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:16:49.369+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:49.379+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:16:49.378+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:16:49.380+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:16:49.402+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.046 seconds
[2024-08-27T19:17:19.744+0000] {processor.py:186} INFO - Started process (PID=133) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:17:19.746+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:17:19.749+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:17:19.748+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:17:19.757+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:17:19.756+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:17:19.757+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:17:19.781+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T19:17:50.125+0000] {processor.py:186} INFO - Started process (PID=134) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:17:50.127+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:17:50.130+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:17:50.130+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:17:50.138+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:17:50.137+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:17:50.139+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:17:50.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.048 seconds
[2024-08-27T19:18:19.543+0000] {processor.py:186} INFO - Started process (PID=135) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:18:19.545+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:18:19.548+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:18:19.547+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:18:19.556+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:18:19.555+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:18:19.556+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:18:19.578+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T19:18:49.854+0000] {processor.py:186} INFO - Started process (PID=136) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:18:49.855+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:18:49.859+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:18:49.858+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:18:49.867+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:18:49.866+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:18:49.868+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:18:49.890+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T19:19:20.203+0000] {processor.py:186} INFO - Started process (PID=137) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:19:20.204+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:19:20.207+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:19:20.207+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:19:20.215+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:19:20.213+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:19:20.216+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:19:20.237+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:19:50.587+0000] {processor.py:186} INFO - Started process (PID=138) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:19:50.588+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:19:50.591+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:19:50.591+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:19:50.598+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:19:50.597+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:19:50.599+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:19:50.620+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.041 seconds
[2024-08-27T19:20:20.961+0000] {processor.py:186} INFO - Started process (PID=139) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:20:20.962+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:20:20.966+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:20:20.966+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:20:20.974+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:20:20.973+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:20:20.975+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:20:20.995+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:20:51.347+0000] {processor.py:186} INFO - Started process (PID=140) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:20:51.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:20:51.350+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:20:51.349+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:20:51.355+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:20:51.354+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:20:51.355+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:20:51.375+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.037 seconds
[2024-08-27T19:21:21.768+0000] {processor.py:186} INFO - Started process (PID=141) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:21:21.770+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:21:21.773+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:21:21.772+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:21:21.780+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:21:21.779+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:21:21.781+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:21:21.801+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.041 seconds
[2024-08-27T19:21:52.150+0000] {processor.py:186} INFO - Started process (PID=142) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:21:52.151+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:21:52.155+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:21:52.154+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:21:52.162+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:21:52.160+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:21:52.162+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:21:52.183+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:22:22.504+0000] {processor.py:186} INFO - Started process (PID=143) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:22:22.505+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:22:22.506+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:22:22.506+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:22:22.510+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:22:22.509+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:22:22.511+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:22:22.523+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T19:22:52.898+0000] {processor.py:186} INFO - Started process (PID=144) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:22:52.899+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:22:52.902+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:22:52.902+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:22:52.909+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:22:52.908+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:22:52.909+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:22:52.931+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.040 seconds
[2024-08-27T19:23:23.249+0000] {processor.py:186} INFO - Started process (PID=145) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:23:23.250+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:23:23.253+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:23:23.252+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:23:23.258+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:23:23.257+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:23:23.258+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:23:23.273+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.029 seconds
[2024-08-27T19:23:53.669+0000] {processor.py:186} INFO - Started process (PID=146) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:23:53.670+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:23:53.673+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:23:53.672+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:23:53.680+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:23:53.679+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:23:53.680+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:23:53.701+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.041 seconds
[2024-08-27T19:24:24.074+0000] {processor.py:186} INFO - Started process (PID=147) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:24:24.076+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:24:24.079+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:24:24.079+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:24:24.087+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:24:24.085+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:24:24.087+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:24:24.110+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.044 seconds
[2024-08-27T19:24:54.482+0000] {processor.py:186} INFO - Started process (PID=148) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:24:54.484+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:24:54.487+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:24:54.486+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:24:54.493+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:24:54.492+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:24:54.494+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:24:54.515+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:25:24.894+0000] {processor.py:186} INFO - Started process (PID=149) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:25:24.896+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:25:24.898+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:25:24.898+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:25:24.906+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:25:24.905+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:25:24.907+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:25:24.929+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.043 seconds
[2024-08-27T19:25:55.253+0000] {processor.py:186} INFO - Started process (PID=150) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:25:55.254+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:25:55.256+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:25:55.256+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:25:55.261+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:25:55.260+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:25:55.262+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:25:55.276+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.028 seconds
[2024-08-27T19:26:25.617+0000] {processor.py:186} INFO - Started process (PID=151) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:26:25.619+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:26:25.621+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:26:25.621+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:26:25.628+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:26:25.627+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:26:25.628+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:26:25.645+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.036 seconds
[2024-08-27T19:26:55.951+0000] {processor.py:186} INFO - Started process (PID=152) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:26:55.952+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:26:55.953+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:26:55.953+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:26:55.957+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:26:55.956+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:26:55.957+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:26:55.970+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.022 seconds
[2024-08-27T19:27:26.394+0000] {processor.py:186} INFO - Started process (PID=153) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:27:26.396+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:27:26.400+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:27:26.399+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:27:26.407+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:27:26.406+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:27:26.408+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:27:26.430+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T19:27:56.783+0000] {processor.py:186} INFO - Started process (PID=154) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:27:56.784+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:27:56.786+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:27:56.786+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:27:56.790+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:27:56.789+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:27:56.791+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:27:56.805+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.027 seconds
[2024-08-27T19:28:27.162+0000] {processor.py:186} INFO - Started process (PID=155) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:28:27.163+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:28:27.166+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:28:27.166+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:28:27.173+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:28:27.172+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:28:27.174+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:28:27.196+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.041 seconds
[2024-08-27T19:28:57.549+0000] {processor.py:186} INFO - Started process (PID=156) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:28:57.550+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:28:57.555+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:28:57.554+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:28:57.563+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:28:57.562+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:28:57.564+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:28:57.585+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.046 seconds
[2024-08-27T19:29:27.946+0000] {processor.py:186} INFO - Started process (PID=157) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:29:27.948+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:29:27.950+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:29:27.950+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:29:27.957+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:29:27.955+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:29:27.958+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:29:27.978+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.040 seconds
[2024-08-27T19:29:58.333+0000] {processor.py:186} INFO - Started process (PID=158) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:29:58.335+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:29:58.339+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:29:58.339+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:29:58.346+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:29:58.345+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:29:58.347+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:29:58.368+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.045 seconds
[2024-08-27T19:30:28.717+0000] {processor.py:186} INFO - Started process (PID=159) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:30:28.718+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:30:28.721+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:30:28.720+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:30:28.727+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:30:28.726+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 7, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
ModuleNotFoundError: No module named 'pipelines'
[2024-08-27T19:30:28.728+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:30:28.748+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.039 seconds
[2024-08-27T19:30:47.869+0000] {processor.py:186} INFO - Started process (PID=160) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:30:47.870+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:30:47.872+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:30:47.871+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:30:47.907+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:30:47.906+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 2, in <module>
    from utils.constants import CLIENT_ID, SECRET
  File "/opt/airflow/utils/constants.py", line 8, in <module>
    CLIENT_ID = parset.get('api_keys', 'reddit_client_id')
NameError: name 'parset' is not defined
[2024-08-27T19:30:47.907+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:30:47.919+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.054 seconds
[2024-08-27T19:31:18.219+0000] {processor.py:186} INFO - Started process (PID=161) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:31:18.220+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:31:18.223+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:31:18.222+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:31:18.267+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:31:18.265+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 2, in <module>
    from utils.constants import CLIENT_ID, SECRET
  File "/opt/airflow/utils/constants.py", line 8, in <module>
    CLIENT_ID = parset.get('api_keys', 'reddit_client_id')
NameError: name 'parset' is not defined
[2024-08-27T19:31:18.267+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:31:18.281+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.070 seconds
[2024-08-27T19:31:48.591+0000] {processor.py:186} INFO - Started process (PID=162) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:31:48.593+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:31:48.595+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:31:48.595+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:31:48.626+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:31:48.625+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 2, in <module>
    from utils.constants import CLIENT_ID, SECRET
  File "/opt/airflow/utils/constants.py", line 8, in <module>
    CLIENT_ID = parset.get('api_keys', 'reddit_client_id')
NameError: name 'parset' is not defined
[2024-08-27T19:31:48.626+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:31:48.641+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.058 seconds
[2024-08-27T19:32:19.041+0000] {processor.py:186} INFO - Started process (PID=163) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:32:19.042+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:32:19.046+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:19.045+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:32:19.081+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:19.080+0000] {dagbag.py:386} ERROR - Failed to import: /opt/airflow/dags/reddit_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 2, in <module>
    from utils.constants import CLIENT_ID, SECRET
  File "/opt/airflow/utils/constants.py", line 8, in <module>
    CLIENT_ID = parset.get('api_keys', 'reddit_client_id')
NameError: name 'parset' is not defined
[2024-08-27T19:32:19.081+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:32:19.094+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.061 seconds
[2024-08-27T19:32:41.171+0000] {processor.py:186} INFO - Started process (PID=26) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:32:41.173+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:32:41.174+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.174+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:32:41.351+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:32:41.431+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.430+0000] {override.py:1900} INFO - Created Permission View: can edit on DAG:etl_reddit_pipeline
[2024-08-27T19:32:41.437+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.436+0000] {override.py:1900} INFO - Created Permission View: can read on DAG:etl_reddit_pipeline
[2024-08-27T19:32:41.440+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.440+0000] {override.py:1900} INFO - Created Permission View: can delete on DAG:etl_reddit_pipeline
[2024-08-27T19:32:41.443+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.443+0000] {override.py:1900} INFO - Created Permission View: can delete on DAG Run:etl_reddit_pipeline
[2024-08-27T19:32:41.446+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.446+0000] {override.py:1900} INFO - Created Permission View: can read on DAG Run:etl_reddit_pipeline
[2024-08-27T19:32:41.449+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.448+0000] {override.py:1900} INFO - Created Permission View: can create on DAG Run:etl_reddit_pipeline
[2024-08-27T19:32:41.451+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.451+0000] {override.py:1900} INFO - Created Permission View: menu access on DAG Run:etl_reddit_pipeline
[2024-08-27T19:32:41.451+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.451+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:32:41.458+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.458+0000] {dag.py:3234} INFO - Creating ORM DAG for etl_reddit_pipeline
[2024-08-27T19:32:41.464+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:32:41.464+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:32:41.474+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.307 seconds
[2024-08-27T19:33:11.610+0000] {processor.py:186} INFO - Started process (PID=27) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:33:11.611+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:33:11.614+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:33:11.613+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:33:11.670+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:33:11.691+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:33:11.691+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:33:11.709+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:33:11.709+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:33:11.719+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.117 seconds
[2024-08-27T19:33:41.953+0000] {processor.py:186} INFO - Started process (PID=28) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:33:41.954+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:33:41.957+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:33:41.957+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:33:42.007+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:33:42.026+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:33:42.026+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:33:42.042+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:33:42.042+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:33:42.055+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.108 seconds
[2024-08-27T19:34:12.491+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:12.492+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:34:12.495+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:12.494+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:12.536+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:12.550+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:12.550+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:34:12.563+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:12.563+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:34:12.572+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.090 seconds
[2024-08-27T19:34:25.710+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:25.712+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:34:25.716+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:25.715+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:25.768+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:25.847+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:25.847+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:34:25.862+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:25.861+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:34:25.875+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.174 seconds
[2024-08-27T19:34:48.675+0000] {processor.py:186} INFO - Started process (PID=26) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:48.676+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:34:48.678+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:48.677+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:48.818+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:34:48.826+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:48.826+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:34:48.839+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:34:48.839+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:34:48.850+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.180 seconds
[2024-08-27T19:35:19.168+0000] {processor.py:186} INFO - Started process (PID=27) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:35:19.169+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:35:19.170+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:35:19.170+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:35:19.197+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:35:19.211+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:35:19.211+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:35:19.222+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:35:19.222+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:35:19.230+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.066 seconds
[2024-08-27T19:35:49.653+0000] {processor.py:186} INFO - Started process (PID=28) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:35:49.654+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:35:49.657+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:35:49.656+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:35:49.709+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:35:49.730+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:35:49.729+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:35:49.745+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:35:49.745+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:35:49.755+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.110 seconds
[2024-08-27T19:36:20.106+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:36:20.107+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:36:20.111+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:36:20.110+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:36:20.152+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:36:20.169+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:36:20.169+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:36:20.184+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:36:20.184+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:36:20.196+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.098 seconds
[2024-08-27T19:36:50.523+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:36:50.524+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:36:50.526+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:36:50.526+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:36:50.571+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:36:50.591+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:36:50.591+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:36:50.604+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:36:50.604+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:36:50.615+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.098 seconds
[2024-08-27T19:37:20.954+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:37:20.955+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:37:20.958+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:37:20.957+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:37:21.003+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:37:21.023+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:37:21.023+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:37:21.036+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:37:21.036+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:37:21.046+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.103 seconds
[2024-08-27T19:37:51.373+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:37:51.374+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:37:51.377+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:37:51.377+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:37:51.422+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:37:51.441+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:37:51.441+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:37:51.454+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:37:51.454+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:37:51.464+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.099 seconds
[2024-08-27T19:38:07.696+0000] {processor.py:186} INFO - Started process (PID=27) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:38:07.697+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:38:07.698+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:38:07.698+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:38:07.857+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:38:07.874+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:38:07.874+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:38:07.885+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:38:07.885+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:38:07.895+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.205 seconds
[2024-08-27T19:38:38.184+0000] {processor.py:186} INFO - Started process (PID=28) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:38:38.186+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:38:38.188+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:38:38.188+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:38:38.237+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:38:38.260+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:38:38.260+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:38:38.277+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:38:38.277+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:38:38.288+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.113 seconds
[2024-08-27T19:39:08.678+0000] {processor.py:186} INFO - Started process (PID=29) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:39:08.680+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:39:08.687+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:39:08.686+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:39:08.736+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:39:08.761+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:39:08.761+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:39:08.780+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:39:08.780+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:39:08.791+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.125 seconds
[2024-08-27T19:39:39.121+0000] {processor.py:186} INFO - Started process (PID=30) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:39:39.122+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:39:39.124+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:39:39.124+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:39:39.170+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:39:39.191+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:39:39.191+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:39:39.205+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:39:39.205+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:39:39.216+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.104 seconds
[2024-08-27T19:40:09.563+0000] {processor.py:186} INFO - Started process (PID=31) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:40:09.564+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:40:09.566+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:40:09.566+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:40:09.597+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:40:09.615+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:40:09.615+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:40:09.627+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:40:09.627+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:40:09.636+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.078 seconds
[2024-08-27T19:40:39.996+0000] {processor.py:186} INFO - Started process (PID=32) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:40:39.997+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:40:40.000+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:40:40.000+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:40:40.049+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:40:40.073+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:40:40.072+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:40:40.086+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:40:40.086+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:40:40.096+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.108 seconds
[2024-08-27T19:41:06.370+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:41:06.372+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:41:06.375+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:41:06.375+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:41:06.429+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:41:06.453+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:41:06.453+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:41:06.469+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:41:06.468+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:41:06.483+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.122 seconds
[2024-08-27T19:41:36.905+0000] {processor.py:186} INFO - Started process (PID=34) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:41:36.906+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:41:36.909+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:41:36.909+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:41:36.955+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:41:36.972+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:41:36.972+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:41:36.986+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:41:36.986+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:41:36.994+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.098 seconds
[2024-08-27T19:42:07.287+0000] {processor.py:186} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:42:07.288+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:42:07.292+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:42:07.292+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:42:07.344+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:42:07.364+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:42:07.364+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:42:07.378+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:42:07.378+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:42:07.387+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.107 seconds
[2024-08-27T19:42:37.691+0000] {processor.py:186} INFO - Started process (PID=36) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:42:37.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:42:37.696+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:42:37.695+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:42:37.738+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:42:37.754+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:42:37.754+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:42:37.766+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:42:37.766+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:42:37.774+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.094 seconds
[2024-08-27T19:43:08.105+0000] {processor.py:186} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:43:08.107+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:43:08.110+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:43:08.110+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:43:08.153+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:43:08.175+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:43:08.175+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:43:08.188+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:43:08.188+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:43:08.198+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.100 seconds
[2024-08-27T19:43:38.540+0000] {processor.py:186} INFO - Started process (PID=38) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:43:38.542+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:43:38.545+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:43:38.544+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:43:38.586+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:43:38.605+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:43:38.604+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:43:38.617+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:43:38.617+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:43:38.627+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.096 seconds
[2024-08-27T19:44:08.987+0000] {processor.py:186} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:44:08.988+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:44:08.990+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:44:08.990+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:44:09.031+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:44:09.050+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:44:09.050+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:44:09.064+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:44:09.064+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:44:09.074+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.093 seconds
[2024-08-27T19:44:39.435+0000] {processor.py:186} INFO - Started process (PID=40) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:44:39.437+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:44:39.440+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:44:39.440+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:44:39.486+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:44:39.506+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:44:39.506+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:44:39.520+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:44:39.519+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:44:39.531+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.106 seconds
[2024-08-27T19:45:09.823+0000] {processor.py:186} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:45:09.824+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:45:09.826+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:45:09.826+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:45:09.867+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:45:09.884+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:45:09.884+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:45:09.897+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:45:09.897+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:45:09.906+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.092 seconds
[2024-08-27T19:45:40.234+0000] {processor.py:186} INFO - Started process (PID=42) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:45:40.235+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:45:40.238+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:45:40.237+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:45:40.282+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:45:40.301+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:45:40.301+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:45:40.314+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:45:40.314+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:45:40.323+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.098 seconds
[2024-08-27T19:46:10.637+0000] {processor.py:186} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:46:10.640+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:46:10.644+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:46:10.644+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:46:10.697+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:46:10.717+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:46:10.717+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:46:10.733+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:46:10.733+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:46:10.746+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.119 seconds
[2024-08-27T19:46:41.066+0000] {processor.py:186} INFO - Started process (PID=44) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:46:41.067+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:46:41.069+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:46:41.068+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:46:41.113+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:46:41.132+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:46:41.131+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:46:41.144+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:46:41.143+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:46:41.152+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.094 seconds
[2024-08-27T19:47:19.034+0000] {processor.py:186} INFO - Started process (PID=27) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:47:19.036+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:47:19.039+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:47:19.039+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:47:19.234+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 569, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:47:19.235+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:47:19.260+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 569, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:47:19.260+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:47:19.365+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:47:19.381+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:47:19.381+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:47:19.395+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:47:19.395+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-26 00:00:00+00:00, run_after=2024-08-27 00:00:00+00:00
[2024-08-27T19:47:19.407+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.382 seconds
[2024-08-27T19:47:49.492+0000] {processor.py:186} INFO - Started process (PID=33) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:47:49.494+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:47:49.497+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:47:49.496+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:47:49.560+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:47:49.561+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:47:49.589+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:47:49.589+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:47:49.683+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:47:49.698+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:47:49.698+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:47:49.710+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:47:49.710+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:47:49.720+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.236 seconds
[2024-08-27T19:48:20.114+0000] {processor.py:186} INFO - Started process (PID=35) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:48:20.116+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:48:20.119+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:48:20.119+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:48:20.182+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:48:20.182+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:48:20.205+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:48:20.205+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:48:20.286+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:48:20.300+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:48:20.300+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:48:20.311+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:48:20.311+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:48:20.318+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.215 seconds
[2024-08-27T19:48:50.505+0000] {processor.py:186} INFO - Started process (PID=37) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:48:50.506+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:48:50.509+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:48:50.509+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:48:50.574+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:48:50.574+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:48:50.596+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:48:50.596+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:48:50.673+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:48:50.688+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:48:50.688+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:48:50.700+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:48:50.700+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:48:50.727+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.232 seconds
[2024-08-27T19:49:21.035+0000] {processor.py:186} INFO - Started process (PID=39) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:49:21.036+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:49:21.041+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:49:21.041+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:49:21.112+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:49:21.113+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:49:21.142+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:49:21.143+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:49:21.234+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:49:21.250+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:49:21.250+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:49:21.262+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:49:21.262+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:49:21.271+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.245 seconds
[2024-08-27T19:49:51.443+0000] {processor.py:186} INFO - Started process (PID=41) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:49:51.445+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:49:51.447+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:49:51.447+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:49:51.505+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:49:51.506+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:49:51.526+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:49:51.526+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:49:51.604+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:49:51.618+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:49:51.618+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:49:51.629+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:49:51.628+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:49:51.636+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-08-27T19:50:21.876+0000] {processor.py:186} INFO - Started process (PID=43) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:50:21.877+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:50:21.881+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:50:21.880+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:50:21.949+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:50:21.950+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:50:21.974+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:50:21.975+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:50:22.058+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:50:22.072+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:50:22.072+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:50:22.084+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:50:22.083+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:50:22.091+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.224 seconds
[2024-08-27T19:50:52.333+0000] {processor.py:186} INFO - Started process (PID=45) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:50:52.334+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:50:52.335+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:50:52.335+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:50:52.376+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:50:52.377+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:50:52.396+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:50:52.396+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:50:52.472+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:50:52.486+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:50:52.486+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:50:52.496+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:50:52.496+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:50:52.503+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.174 seconds
[2024-08-27T19:51:22.863+0000] {processor.py:186} INFO - Started process (PID=47) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:51:22.865+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:51:22.868+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:51:22.867+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:51:22.922+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:51:22.923+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:51:22.942+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:51:22.942+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:51:23.016+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:51:23.029+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:51:23.029+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:51:23.040+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:51:23.039+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:51:23.048+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.194 seconds
[2024-08-27T19:51:53.272+0000] {processor.py:186} INFO - Started process (PID=49) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:51:53.273+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:51:53.276+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:51:53.276+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:51:53.329+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:51:53.329+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:51:53.348+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:51:53.348+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:51:53.423+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:51:53.436+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:51:53.436+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:51:53.446+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:51:53.446+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:51:53.454+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.191 seconds
[2024-08-27T19:52:23.808+0000] {processor.py:186} INFO - Started process (PID=51) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:52:23.810+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:52:23.813+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:52:23.813+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:52:23.868+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:52:23.868+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:52:23.889+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:52:23.889+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:52:23.962+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:52:23.976+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:52:23.976+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:52:23.986+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:52:23.986+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:52:23.993+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.195 seconds
[2024-08-27T19:52:54.220+0000] {processor.py:186} INFO - Started process (PID=53) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:52:54.221+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:52:54.224+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:52:54.224+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:52:54.280+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:52:54.280+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:52:54.299+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:52:54.299+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:52:54.372+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:52:54.385+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:52:54.385+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:52:54.395+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:52:54.395+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:52:54.403+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.192 seconds
[2024-08-27T19:53:24.717+0000] {processor.py:186} INFO - Started process (PID=55) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:53:24.718+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:53:24.722+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:53:24.722+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:53:24.778+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:53:24.778+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:53:24.798+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:53:24.798+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:53:24.873+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:53:24.887+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:53:24.887+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:53:24.897+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:53:24.897+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:53:24.905+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-08-27T19:53:55.094+0000] {processor.py:186} INFO - Started process (PID=57) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:53:55.095+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:53:55.098+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:53:55.097+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:53:55.152+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:53:55.152+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:53:55.172+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:53:55.173+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:53:55.247+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:53:55.262+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:53:55.261+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:53:55.271+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:53:55.271+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:53:55.279+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.193 seconds
[2024-08-27T19:54:25.656+0000] {processor.py:186} INFO - Started process (PID=59) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:54:25.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:54:25.659+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:54:25.659+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:54:25.708+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:54:25.709+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:54:25.728+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:54:25.728+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:54:25.802+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:54:25.816+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:54:25.816+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:54:25.826+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:54:25.826+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:54:25.834+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.185 seconds
[2024-08-27T19:54:56.059+0000] {processor.py:186} INFO - Started process (PID=61) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:54:56.061+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:54:56.064+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:54:56.064+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:54:56.121+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:54:56.121+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:54:56.140+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:54:56.140+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:54:56.217+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:54:56.230+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:54:56.230+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:54:56.240+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:54:56.240+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:54:56.246+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-08-27T19:55:26.595+0000] {processor.py:186} INFO - Started process (PID=63) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:55:26.597+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:55:26.600+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:55:26.599+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:55:26.656+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:55:26.656+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:55:26.676+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:55:26.676+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:55:26.749+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:55:26.763+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:55:26.763+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:55:26.773+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:55:26.773+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:55:26.780+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.194 seconds
[2024-08-27T19:55:57.045+0000] {processor.py:186} INFO - Started process (PID=65) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:55:57.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:55:57.050+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:55:57.049+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:55:57.115+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:55:57.115+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:55:57.141+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:55:57.142+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:55:57.224+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:55:57.239+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:55:57.239+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:55:57.251+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:55:57.251+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:55:57.258+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.223 seconds
[2024-08-27T19:56:27.632+0000] {processor.py:186} INFO - Started process (PID=67) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:56:27.633+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:56:27.636+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:56:27.636+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:56:27.730+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:56:27.731+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:56:27.757+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:56:27.757+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:56:27.842+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:56:27.857+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:56:27.857+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:56:27.868+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:56:27.868+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:56:27.876+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.253 seconds
[2024-08-27T19:56:58.017+0000] {processor.py:186} INFO - Started process (PID=69) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:56:58.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:56:58.020+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:56:58.019+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:56:58.072+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:56:58.073+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:56:58.094+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:56:58.095+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:56:58.174+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:56:58.189+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:56:58.189+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:56:58.199+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:56:58.199+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:56:58.207+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.198 seconds
[2024-08-27T19:57:28.510+0000] {processor.py:186} INFO - Started process (PID=71) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:57:28.512+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:57:28.516+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:57:28.515+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:57:28.582+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:57:28.583+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:57:28.606+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:57:28.607+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:57:28.688+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:57:28.702+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:57:28.702+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:57:28.713+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:57:28.713+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:57:28.720+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.219 seconds
[2024-08-27T19:57:58.952+0000] {processor.py:186} INFO - Started process (PID=73) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:57:58.953+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:57:58.956+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:57:58.956+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:57:59.012+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:57:59.013+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:57:59.032+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:57:59.032+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:57:59.111+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:57:59.126+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:57:59.125+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:57:59.136+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:57:59.136+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:57:59.143+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.200 seconds
[2024-08-27T19:58:29.478+0000] {processor.py:186} INFO - Started process (PID=75) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:58:29.479+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:58:29.482+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:58:29.481+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:58:29.548+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:58:29.548+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:58:29.574+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:58:29.574+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:58:29.655+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:58:29.673+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:58:29.673+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:58:29.686+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:58:29.686+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:58:29.693+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-08-27T19:58:59.866+0000] {processor.py:186} INFO - Started process (PID=77) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:58:59.867+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:58:59.869+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:58:59.868+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:58:59.913+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:58:59.913+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:58:59.935+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:58:59.935+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:59:00.019+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:59:00.034+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:59:00.034+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:59:00.045+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:59:00.045+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:59:00.053+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.192 seconds
[2024-08-27T19:59:30.451+0000] {processor.py:186} INFO - Started process (PID=79) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:59:30.452+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T19:59:30.455+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:59:30.455+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:59:30.510+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:59:30.510+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:59:30.529+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T19:59:30.530+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T19:59:30.606+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T19:59:30.619+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:59:30.619+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T19:59:30.630+0000] {logging_mixin.py:190} INFO - [2024-08-27T19:59:30.630+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T19:59:30.637+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.195 seconds
[2024-08-27T20:00:00.880+0000] {processor.py:186} INFO - Started process (PID=81) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:00:00.882+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:00:00.884+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:00:00.884+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:00:00.937+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:00:00.938+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:00:00.959+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:00:00.959+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:00:01.037+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:00:01.051+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:00:01.050+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:00:01.062+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:00:01.062+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:00:01.069+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.198 seconds
[2024-08-27T20:00:31.371+0000] {processor.py:186} INFO - Started process (PID=83) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:00:31.374+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:00:31.380+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:00:31.380+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:00:31.440+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:00:31.440+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:00:31.468+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:00:31.471+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:00:31.560+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:00:31.577+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:00:31.577+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:00:31.591+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:00:31.591+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:00:31.602+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.236 seconds
[2024-08-27T20:01:01.776+0000] {processor.py:186} INFO - Started process (PID=85) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:01:01.777+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:01:01.781+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:01:01.781+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:01:01.855+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:01:01.856+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:01:01.885+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:01:01.885+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:01:01.984+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:01:02.002+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:01:02.002+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:01:02.016+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:01:02.016+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:01:02.025+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.257 seconds
[2024-08-27T20:01:32.396+0000] {processor.py:186} INFO - Started process (PID=87) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:01:32.398+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:01:32.401+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:01:32.400+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:01:32.461+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:01:32.462+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:01:32.483+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:01:32.483+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:01:32.563+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:01:32.577+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:01:32.577+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:01:32.587+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:01:32.587+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:01:32.594+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.207 seconds
[2024-08-27T20:02:02.866+0000] {processor.py:186} INFO - Started process (PID=89) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:02:02.868+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:02:02.871+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:02:02.871+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:02:02.956+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:02:02.957+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:02:02.986+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:02:02.987+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:02:03.090+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:02:03.107+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:02:03.107+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:02:03.122+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:02:03.121+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:02:03.132+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.275 seconds
[2024-08-27T20:02:33.344+0000] {processor.py:186} INFO - Started process (PID=91) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:02:33.345+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:02:33.348+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:02:33.347+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:02:33.410+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:02:33.411+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:02:33.432+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:02:33.433+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:02:33.511+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:02:33.526+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:02:33.526+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:02:33.537+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:02:33.537+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:02:33.543+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.209 seconds
[2024-08-27T20:03:03.835+0000] {processor.py:186} INFO - Started process (PID=93) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:03:03.836+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:03:03.840+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:03:03.839+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:03:03.904+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:03:03.904+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:03:03.930+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:03:03.931+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:03:04.017+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:03:04.033+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:03:04.032+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:03:04.044+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:03:04.044+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:03:04.053+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.226 seconds
[2024-08-27T20:03:34.181+0000] {processor.py:186} INFO - Started process (PID=95) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:03:34.182+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:03:34.185+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:03:34.185+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:03:34.240+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:03:34.241+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:03:34.263+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:03:34.263+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:03:34.347+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:03:34.361+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:03:34.361+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:03:34.372+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:03:34.372+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:03:34.380+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.208 seconds
[2024-08-27T20:04:04.540+0000] {processor.py:186} INFO - Started process (PID=97) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:04:04.541+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:04:04.544+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:04:04.544+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:04:04.611+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:04:04.612+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:04:04.638+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:04:04.639+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:04:04.724+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:04:04.738+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:04:04.738+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:04:04.750+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:04:04.750+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:04:04.757+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-08-27T20:04:35.086+0000] {processor.py:186} INFO - Started process (PID=99) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:04:35.087+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:04:35.091+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:04:35.090+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:04:35.147+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:04:35.148+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:04:35.168+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:04:35.168+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:04:35.245+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:04:35.260+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:04:35.260+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:04:35.270+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:04:35.270+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:04:35.278+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.201 seconds
[2024-08-27T20:05:05.465+0000] {processor.py:186} INFO - Started process (PID=101) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:05:05.466+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:05:05.469+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:05:05.468+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:05:05.526+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:05:05.526+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:05:05.546+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:05:05.547+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:05:05.624+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:05:05.637+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:05:05.637+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:05:05.647+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:05:05.647+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:05:05.655+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.200 seconds
[2024-08-27T20:05:36.024+0000] {processor.py:186} INFO - Started process (PID=103) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:05:36.025+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:05:36.028+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:05:36.028+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:05:36.086+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:05:36.087+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:05:36.107+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:05:36.107+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:05:36.182+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:05:36.196+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:05:36.196+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:05:36.207+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:05:36.207+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:05:36.214+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.201 seconds
[2024-08-27T20:06:06.418+0000] {processor.py:186} INFO - Started process (PID=105) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:06:06.419+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:06:06.422+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:06:06.421+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:06:06.471+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:06:06.472+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:06:06.492+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:06:06.493+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:06:06.576+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:06:06.590+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:06:06.590+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:06:06.602+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:06:06.602+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:06:06.610+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.199 seconds
[2024-08-27T20:06:36.955+0000] {processor.py:186} INFO - Started process (PID=107) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:06:36.956+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:06:36.959+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:06:36.959+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:06:37.029+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:06:37.030+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:06:37.057+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:06:37.057+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:06:37.145+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:06:37.160+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:06:37.159+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:06:37.171+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:06:37.171+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:06:37.178+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.234 seconds
[2024-08-27T20:07:07.351+0000] {processor.py:186} INFO - Started process (PID=109) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:07:07.353+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:07:07.356+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:07:07.356+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:07:07.433+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:07:07.434+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:07:07.462+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:07:07.462+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:07:07.557+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:07:07.574+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:07:07.573+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:07:07.586+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:07:07.586+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:07:07.595+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.254 seconds
[2024-08-27T20:07:37.936+0000] {processor.py:186} INFO - Started process (PID=111) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:07:37.937+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:07:37.940+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:07:37.939+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:07:37.995+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:07:37.995+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:07:38.016+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:07:38.016+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:07:38.095+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:07:38.109+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:07:38.109+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:07:38.119+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:07:38.119+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:07:38.127+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-08-27T20:08:08.308+0000] {processor.py:186} INFO - Started process (PID=113) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:08:08.309+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:08:08.312+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:08:08.312+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:08:08.369+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:08:08.370+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:08:08.389+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:08:08.389+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:08:08.465+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:08:08.479+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:08:08.479+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:08:08.489+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:08:08.489+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:08:08.497+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.200 seconds
[2024-08-27T20:08:38.829+0000] {processor.py:186} INFO - Started process (PID=115) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:08:38.831+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:08:38.835+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:08:38.834+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:08:38.891+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:08:38.891+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:08:38.913+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:08:38.914+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:08:38.999+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:08:39.016+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:08:39.016+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:08:39.029+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:08:39.029+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:08:39.037+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.217 seconds
[2024-08-27T20:09:09.211+0000] {processor.py:186} INFO - Started process (PID=117) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:09:09.212+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:09:09.214+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:09:09.214+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:09:09.267+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:09:09.267+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:09:09.290+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:09:09.290+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:09:09.378+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:09:09.394+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:09:09.394+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:09:09.405+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:09:09.405+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:09:09.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.209 seconds
[2024-08-27T20:09:39.774+0000] {processor.py:186} INFO - Started process (PID=119) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:09:39.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:09:39.779+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:09:39.778+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:09:39.849+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:09:39.850+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:09:39.876+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:09:39.877+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:09:39.963+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:09:39.978+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:09:39.977+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:09:39.989+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:09:39.989+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:09:39.998+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.232 seconds
[2024-08-27T20:10:10.120+0000] {processor.py:186} INFO - Started process (PID=121) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:10:10.121+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:10:10.124+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:10:10.124+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:10:10.179+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:10:10.179+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:10:10.202+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:10:10.203+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:10:10.284+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:10:10.298+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:10:10.298+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:10:10.308+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:10:10.308+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:10:10.316+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-08-27T20:10:40.679+0000] {processor.py:186} INFO - Started process (PID=123) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:10:40.680+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:10:40.684+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:10:40.683+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:10:40.745+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:10:40.746+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:10:40.769+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:10:40.769+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:10:40.854+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:10:40.871+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:10:40.871+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:10:40.884+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:10:40.884+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:10:40.894+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.225 seconds
[2024-08-27T20:11:11.025+0000] {processor.py:186} INFO - Started process (PID=125) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:11:11.026+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:11:11.030+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:11:11.029+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:11:11.089+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:11:11.089+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:11:11.109+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:11:11.110+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:11:11.186+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:11:11.201+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:11:11.201+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:11:11.212+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:11:11.212+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:11:11.221+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.205 seconds
[2024-08-27T20:11:41.642+0000] {processor.py:186} INFO - Started process (PID=127) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:11:41.643+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:11:41.646+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:11:41.646+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:11:41.699+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:11:41.699+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:11:41.719+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:11:41.719+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:11:41.798+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:11:41.813+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:11:41.813+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:11:41.823+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:11:41.823+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:11:41.830+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.196 seconds
[2024-08-27T20:12:12.033+0000] {processor.py:186} INFO - Started process (PID=129) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:12:12.034+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:12:12.036+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:12:12.035+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:12:12.100+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:12:12.100+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:12:12.124+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:12:12.125+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:12:12.211+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:12:12.226+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:12:12.226+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:12:12.238+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:12:12.238+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:12:12.247+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.221 seconds
[2024-08-27T20:12:42.562+0000] {processor.py:186} INFO - Started process (PID=131) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:12:42.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:12:42.567+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:12:42.566+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:12:42.634+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:12:42.635+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:12:42.663+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:12:42.663+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:12:42.770+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:12:42.788+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:12:42.788+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:12:42.802+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:12:42.802+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:12:42.811+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.257 seconds
[2024-08-27T20:13:12.945+0000] {processor.py:186} INFO - Started process (PID=133) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:13:12.946+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:13:12.949+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:13:12.948+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:13:12.998+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:13:12.999+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:13:13.021+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:13:13.021+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:13:13.102+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:13:13.116+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:13:13.116+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:13:13.126+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:13:13.126+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:13:13.134+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.195 seconds
[2024-08-27T20:13:43.525+0000] {processor.py:186} INFO - Started process (PID=135) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:13:43.526+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:13:43.529+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:13:43.529+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:13:43.593+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:13:43.593+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:13:43.616+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:13:43.617+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:13:43.702+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:13:43.720+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:13:43.720+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:13:43.733+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:13:43.733+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:13:43.744+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.229 seconds
[2024-08-27T20:14:13.961+0000] {processor.py:186} INFO - Started process (PID=137) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:14:13.962+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:14:13.965+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:14:13.965+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:14:14.020+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:14:14.021+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:14:14.041+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:14:14.041+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:14:14.123+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:14:14.137+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:14:14.137+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:14:14.148+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:14:14.148+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:14:14.156+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.204 seconds
[2024-08-27T20:14:44.466+0000] {processor.py:186} INFO - Started process (PID=139) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:14:44.468+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:14:44.471+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:14:44.471+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:14:44.535+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:14:44.535+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:14:44.559+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:14:44.560+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:14:44.645+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:14:44.659+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:14:44.658+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:14:44.671+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:14:44.671+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:14:44.680+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.222 seconds
[2024-08-27T20:15:14.888+0000] {processor.py:186} INFO - Started process (PID=141) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:15:14.889+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:15:14.893+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:15:14.892+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:15:14.964+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:15:14.965+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:15:14.993+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:15:14.994+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:15:15.083+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:15:15.100+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:15:15.099+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:15:15.112+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:15:15.112+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:15:15.121+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.243 seconds
[2024-08-27T20:15:45.383+0000] {processor.py:186} INFO - Started process (PID=143) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:15:45.384+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:15:45.386+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:15:45.386+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:15:45.432+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:15:45.432+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:15:45.453+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:15:45.453+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:15:45.529+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:15:45.543+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:15:45.543+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:15:45.553+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:15:45.553+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:15:45.561+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.184 seconds
[2024-08-27T20:16:15.863+0000] {processor.py:186} INFO - Started process (PID=145) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:16:15.864+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:16:15.867+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:16:15.867+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:16:15.920+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:16:15.920+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:16:15.940+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:16:15.940+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:16:16.019+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:16:16.033+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:16:16.033+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:16:16.043+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:16:16.043+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:16:16.051+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.196 seconds
[2024-08-27T20:16:46.320+0000] {processor.py:186} INFO - Started process (PID=147) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:16:46.321+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:16:46.325+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:16:46.324+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:16:46.382+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:16:46.382+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:16:46.402+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:16:46.402+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:16:46.476+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:16:46.490+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:16:46.490+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:16:46.500+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:16:46.500+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:16:46.508+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-08-27T20:17:16.817+0000] {processor.py:186} INFO - Started process (PID=149) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:17:16.819+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:17:16.823+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:17:16.823+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:17:16.893+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:17:16.894+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:17:16.923+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:17:16.923+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:17:17.015+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:17:17.029+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:17:17.029+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:17:17.040+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:17:17.040+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:17:17.049+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.241 seconds
[2024-08-27T20:17:47.191+0000] {processor.py:186} INFO - Started process (PID=151) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:17:47.192+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:17:47.196+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:17:47.195+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:17:47.270+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:17:47.270+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:17:47.298+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:17:47.299+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:17:47.398+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:17:47.414+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:17:47.414+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:17:47.427+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:17:47.427+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:17:47.436+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.253 seconds
[2024-08-27T20:18:17.785+0000] {processor.py:186} INFO - Started process (PID=153) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:18:17.786+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:18:17.790+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:18:17.790+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:18:17.851+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:18:17.852+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:18:17.873+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:18:17.873+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:18:17.952+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:18:17.967+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:18:17.967+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:18:17.978+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:18:17.977+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:18:17.985+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.211 seconds
[2024-08-27T20:18:48.176+0000] {processor.py:186} INFO - Started process (PID=155) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:18:48.178+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:18:48.180+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:18:48.180+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:18:48.237+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:18:48.238+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:18:48.258+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:18:48.258+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:18:48.335+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:18:48.348+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:18:48.348+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:18:48.358+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:18:48.358+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:18:48.365+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-08-27T20:19:18.728+0000] {processor.py:186} INFO - Started process (PID=157) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:19:18.729+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:19:18.733+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:19:18.732+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:19:18.793+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:19:18.793+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:19:18.814+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:19:18.815+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:19:18.894+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:19:18.908+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:19:18.908+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:19:18.918+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:19:18.918+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:19:18.926+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.207 seconds
[2024-08-27T20:19:49.105+0000] {processor.py:186} INFO - Started process (PID=159) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:19:49.106+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:19:49.110+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:19:49.109+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:19:49.163+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:19:49.164+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:19:49.184+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:19:49.184+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:19:49.258+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:19:49.272+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:19:49.271+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:19:49.281+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:19:49.281+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:19:49.289+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.193 seconds
[2024-08-27T20:20:19.568+0000] {processor.py:186} INFO - Started process (PID=161) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:20:19.570+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:20:19.573+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:20:19.572+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:20:19.630+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:20:19.630+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:20:19.652+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:20:19.652+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:20:19.730+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:20:19.744+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:20:19.744+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:20:19.755+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:20:19.755+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:20:19.762+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.202 seconds
[2024-08-27T20:20:49.977+0000] {processor.py:186} INFO - Started process (PID=163) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:20:49.978+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:20:49.982+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:20:49.981+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:20:50.039+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:20:50.039+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:20:50.058+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:20:50.058+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:20:50.133+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:20:50.147+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:20:50.147+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:20:50.157+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:20:50.157+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:20:50.164+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.197 seconds
[2024-08-27T20:21:20.429+0000] {processor.py:186} INFO - Started process (PID=165) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:21:20.430+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:21:20.432+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:21:20.432+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:21:20.486+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:21:20.487+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:21:20.512+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:21:20.512+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:21:20.602+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:21:20.619+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:21:20.619+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:21:20.632+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:21:20.632+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:21:20.641+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.219 seconds
[2024-08-27T20:21:50.842+0000] {processor.py:186} INFO - Started process (PID=167) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:21:50.844+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:21:50.847+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:21:50.846+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:21:50.909+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:21:50.910+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:21:50.932+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:21:50.933+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:21:51.016+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:21:51.031+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:21:51.031+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:21:51.042+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:21:51.042+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:21:51.051+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.219 seconds
[2024-08-27T20:22:21.356+0000] {processor.py:186} INFO - Started process (PID=169) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:22:21.357+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:22:21.361+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:22:21.360+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:22:21.425+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:22:21.426+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:22:21.453+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:22:21.454+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:22:21.541+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:22:21.556+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:22:21.556+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:22:21.567+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:22:21.567+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:22:21.574+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.227 seconds
[2024-08-27T20:22:51.804+0000] {processor.py:186} INFO - Started process (PID=171) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:22:51.806+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:22:51.809+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:22:51.808+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:22:51.873+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:22:51.874+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:22:51.901+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:22:51.902+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:22:51.989+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:22:52.005+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:22:52.005+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:22:52.016+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:22:52.016+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:22:52.024+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.229 seconds
[2024-08-27T20:23:22.313+0000] {processor.py:186} INFO - Started process (PID=173) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:23:22.314+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:23:22.318+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:23:22.317+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:23:22.377+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:23:22.378+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:23:22.399+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:23:22.400+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:23:22.482+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:23:22.497+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:23:22.497+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:23:22.508+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:23:22.508+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:23:22.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.211 seconds
[2024-08-27T20:23:52.683+0000] {processor.py:186} INFO - Started process (PID=175) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:23:52.684+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:23:52.687+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:23:52.686+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:23:52.746+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:23:52.747+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:23:52.770+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:23:52.770+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:23:52.852+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:23:52.868+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:23:52.868+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:23:52.879+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:23:52.879+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:23:52.887+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.214 seconds
[2024-08-27T20:24:23.264+0000] {processor.py:186} INFO - Started process (PID=177) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:24:23.265+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:24:23.268+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:24:23.267+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:24:23.326+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:24:23.327+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:24:23.351+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:24:23.351+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:24:23.434+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:24:23.449+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:24:23.449+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:24:23.460+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:24:23.460+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:24:23.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.212 seconds
[2024-08-27T20:24:53.671+0000] {processor.py:186} INFO - Started process (PID=179) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:24:53.672+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:24:53.676+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:24:53.676+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:24:53.748+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:24:53.749+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:24:53.776+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:24:53.776+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:24:53.866+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:24:53.882+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:24:53.881+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:24:53.893+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:24:53.893+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:24:53.901+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.241 seconds
[2024-08-27T20:25:24.179+0000] {processor.py:186} INFO - Started process (PID=181) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:25:24.180+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:25:24.183+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:25:24.183+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:25:24.243+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:25:24.244+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:25:24.267+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:25:24.268+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:25:24.355+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:25:24.372+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:25:24.372+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:25:24.386+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:25:24.386+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:25:24.395+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.228 seconds
[2024-08-27T20:25:54.640+0000] {processor.py:186} INFO - Started process (PID=183) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:25:54.641+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:25:54.645+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:25:54.644+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:25:54.729+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:25:54.730+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:25:54.766+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:25:54.770+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:25:54.875+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:25:54.892+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:25:54.892+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:25:54.905+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:25:54.905+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:25:54.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.284 seconds
[2024-08-27T20:26:25.120+0000] {processor.py:186} INFO - Started process (PID=185) to work on /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:26:25.121+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/reddit_dag.py for tasks to queue
[2024-08-27T20:26:25.125+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:26:25.125+0000] {dagbag.py:587} INFO - Filling up the DagBag from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:26:25.181+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 26, in <module>
    from pandas.compat import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/__init__.py", line 27, in <module>
    from pandas.compat.pyarrow import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:26:25.181+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:26:25.201+0000] {logging_mixin.py:190} WARNING - 
A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/airflow/.local/bin/airflow", line 8, in <module>
    sys.exit(main())
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/__main__.py", line 62, in main
    args.func(args)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/cli.py", line 115, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/providers_configuration_loader.py", line 55, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 59, in scheduler
    run_command_with_daemon_option(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/daemon_utils.py", line 86, in run_command_with_daemon_option
    callback()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 62, in <lambda>
    callback=lambda: _run_scheduler_job(args),
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/cli/commands/scheduler_command.py", line 48, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 421, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/job.py", line 450, in execute_job
    ret = execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/jobs/scheduler_job_runner.py", line 980, in _execute
    self.processor_agent.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 172, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 247, in _run_processor_manager
    processor_manager.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 489, in start
    return self._run_parsing_loop()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 651, in _run_parsing_loop
    self.start_new_processes()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/traces/tracer.py", line 58, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/manager.py", line 1239, in start_new_processes
    processor.start()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 263, in start
    process.start()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/usr/local/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/usr/local/lib/python3.9/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/usr/local/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 207, in _run_file_processor
    _handle_dag_file_processing()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 188, in _handle_dag_file_processing
    result: tuple[int, int, int] = dag_file_processor.process_file(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 918, in process_file
    dagbag = DagFileProcessor._get_dagbag(file_path)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/dag_processing/processor.py", line 882, in _get_dagbag
    return DagBag(file_path, include_examples=False)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 172, in __init__
    self.collect_dags(
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 601, in collect_dags
    found_dags = self.process_file(filepath, only_if_updated=only_if_updated, safe_mode=safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 340, in process_file
    mods = self._load_modules_from_file(filepath, safe_mode)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 412, in _load_modules_from_file
    return parse(mod_name, filepath)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/dagbag.py", line 382, in parse
    loader.exec_module(new_module)
  File "/opt/airflow/dags/reddit_dag.py", line 9, in <module>
    from pipelines.reddit_pipeline import reddit_pipeline
  File "/opt/airflow/pipelines/reddit_pipeline.py", line 1, in <module>
    from etls.reddit_etl import connect_reddit, extract_posts, load_data_to_csv, tranform_data
  File "/opt/airflow/etls/reddit_etl.py", line 6, in <module>
    import pandas as pd
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/__init__.py", line 49, in <module>
    from pandas.core.api import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/api.py", line 9, in <module>
    from pandas.core.dtypes.dtypes import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pandas/core/dtypes/dtypes.py", line 24, in <module>
    from pandas._libs import (
  File "/home/airflow/.local/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
[2024-08-27T20:26:25.202+0000] {logging_mixin.py:190} WARNING - AttributeError: _ARRAY_API not found
[2024-08-27T20:26:25.282+0000] {processor.py:925} INFO - DAG(s) 'etl_reddit_pipeline' retrieved from /opt/airflow/dags/reddit_dag.py
[2024-08-27T20:26:25.297+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:26:25.297+0000] {dag.py:3211} INFO - Sync 1 DAGs
[2024-08-27T20:26:25.308+0000] {logging_mixin.py:190} INFO - [2024-08-27T20:26:25.308+0000] {dag.py:4138} INFO - Setting next_dagrun for etl_reddit_pipeline to 2024-08-27 00:00:00+00:00, run_after=2024-08-28 00:00:00+00:00
[2024-08-27T20:26:25.317+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/reddit_dag.py took 0.208 seconds
